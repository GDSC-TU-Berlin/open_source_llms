{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Downloaden der GGUF Modelle\n",
    "Um zum ersten Mal ein LLM bei uns auszuführen, wollen wir zunächst ein gguf Modelle verwenden. Für den Anfang verwenden wir das Llama-2-7B-GGUF und das Mistral-7B-Instruct-v0.1-GGUF Modell. Diese Modelle sind relativ klein und können direkt auf einer CPU ausgeführt werden. Um diser Modelle zu verwenden, müssen wir diese zunächst herunterladen. Dafür verwenden wir die Funktion hf_hub_download aus dem huggingface_hub Modul um die Modelle von huggingface herunterzuladen. Weitere gguf Modelle können zum Beispiel  [hier](https://huggingface.co/TheBloke) gefunden werden."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3369a0a1ee208f8a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "llama_path = hf_hub_download(repo_id=\"TheBloke/Llama-2-7B-GGUF\", filename=\"llama-2-7b.Q4_0.gguf\")\n",
    "mistral_path = hf_hub_download(repo_id=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "                               filename=\"mistral-7b-instruct-v0.1.Q4_0.gguf\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f883f132125d01d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ausführen der gguf Modelle\n",
    "Nach dem herunterladen der Modelle, können wir diese nun in unserem Python Code verwenden. Dafür verwenden wir die llama-cpp-python Bibliothek, die uns eine einfache Schnittstelle zu den Modellen bietet. Um ein Modell zu verwenden, müssen wir zunächst eine Instanz der Llama Klasse erstellen und den Pfad zum Modell übergeben. Anschließend können wir die Instanz wie eine Funktion aufrufen und die Eingabe (Prompt) übergeben. Die Ausgabe ist ein Python Dictionary, das die Antwort des Modells enthält."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20dc170c4f08a75d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llama = Llama(llama_path)\n",
    "response = llama(\"What is the capital of France?\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e36ffe6542e1dae",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die Ausgabe ist eine Python Map. Wenn wir diese etwas schöner formattieren könnte die Ausgabe wie folgt aussehen.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"cmpl-97de95d4-101f-4193-a544-65d26211ddba\",\n",
    "  \"object\": \"text_completion\",\n",
    "  \"created\": 1707901428,\n",
    "  \"model\": \"/home/jt/.cache/huggingface/hub/models--TheBloke--Llama-2-7B-GGUF/snapshots/b4e04e128f421c93a5f1e34ac4d7ca9b0af47b80/llama-2-7b.Q4_0.gguf\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"text\": \"1. hopefully you got a good mark in your Geography lesson today!\\n\",\n",
    "      \"index\": 0,\n",
    "      \"logprobs\": \"None\",\n",
    "      \"finish_reason\": \"length\"\n",
    "    }\n",
    "  ],\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 13,\n",
    "    \"completion_tokens\": 16,\n",
    "    \"total_tokens\": 29\n",
    "  }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bedf39529f332c92"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zugriff auf die Antwort"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12706d37d00185ab"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mresponse\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchoices\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T14:02:33.467374600Z",
     "start_time": "2024-02-15T14:02:33.425149740Z"
    }
   },
   "id": "e2098d4af27e8451",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model mit GPU Unterstützung\n",
    "Aktuell ist unsere Modell noch etwas langsam um dies nun zu beschleunigen wollen wir eine GPU nutzen. Um unser Model auf der GPU auszuführen, müssen wir zunächst den Laufzeit-Typ auf GPU umstellen. Dafür klicken wir in der Toolbar unter Laufzeit auf Laufzeittyp ändern und wählen nun die T4 GPU aus. Anschließend installieren wir llama-cpp-python erneut, diesmal mit der Option -DLLAMA_CUBLAS=on um CUDA zu aktivieren."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d1d73ac514f9e16"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46dd86fda36fa972",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llama = Llama(\n",
    "    llama_path,\n",
    "    n_gpu_layers=-1, # Verschiebt die Berechnung auf die GPU\n",
    "    verbose=False, # Entfernt die Logausgaben\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84d814bf34d0bf1b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = llama(\"What is the capital of France?\")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e21cc17d5b75dbec",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bessere Antworten bekommen\n",
    "Das Llama Model was wir verwenden ist ein sogenanntes Foundation Model. Diese Modelle sind meißt nicht direkt in der Anwendung nutzbar, sondern müssen noch finegetuned werden. Ein soein gefinetuntes Modell ist das Mistral-7B-Instruct-v0.1-GGUF Modell. Dieses Modell ist speziell darauf trainiert worden, Anweisungen zu befolgen. Dieses Modell haben wir bereits heruntergeladen und können es nun verwenden. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "677ff76b567bbfe0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mistral = Llama(\n",
    "    mistral_path,\n",
    "    n_gpu_layers=-1, \n",
    "    verbose=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef5715cb6adc58b3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = mistral(\"What is the capital of France?\")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9dc443f3557e7e9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = mistral(\n",
    "    \"Write me a poem about the technology of the future.\",\n",
    "    max_tokens=512,\n",
    ")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3129491fd3dc1030",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chatbot erstellung"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f09497ddac5d0a98"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "current_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"answer in the style of a pirate\",\n",
    "    }\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "    current_messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_input,\n",
    "    })\n",
    "    \n",
    "    response = mistral.create_chat_completion(current_messages, max_tokens=512)\n",
    "    current_messages.append(response[\"choices\"][0][\"message\"])\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1be6db17042728f7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Streaming der Antworten"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93f668bec992446f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = mistral(\n",
    "    \"Write me a poem about the technology of the future.\",\n",
    "    max_tokens=512,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for message in response:\n",
    "    print(message[\"choices\"][0][\"text\"], end=\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae44560be9333686",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aufgabe\n",
    "baut das Streaming in unseren Chatbot ein"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fa8ccae2f93dd5b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "current_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"answer in the style of a pirate\",\n",
    "    }\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "    current_messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_input,\n",
    "    })\n",
    "    \n",
    "    response = mistral.create_chat_completion(\n",
    "        current_messages, \n",
    "        max_tokens=512,\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    message = \"\"\n",
    "    for chunk in response:\n",
    "        part = chunk[\"choices\"][0][\"message\"][\"content\"]\n",
    "        message += part\n",
    "        print(part, end=\"\")\n",
    "    \n",
    "    current_messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": message,\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ac816855b548835"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exllamav2\n",
    "Wir wollen uns nun eine weiter Quantisierung von LLM Modellen anschauen. Dafür verwenden wir das Exllamav2, welches exl2 Modelle verwendet.\n",
    "\n",
    "Das besondere an Exllamav2 ist, ist das ist sehr schnell ist, weshalb wir es uns nun im folgenden anschauen wollen. Zunächst müssten wir hierfür exllamav2 installieren und ein Modell herunterladen. Viele Modelle können zum Beispiel [hier](https://huggingface.co/LoneStriker) gefunden werden."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3970fbc44634d7d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: exllamav2 in ./venv/lib/python3.11/site-packages (0.0.13.post2)\r\n",
      "Requirement already satisfied: pandas in ./venv/lib64/python3.11/site-packages (from exllamav2) (2.2.0)\r\n",
      "Requirement already satisfied: ninja in ./venv/lib64/python3.11/site-packages (from exllamav2) (1.11.1.1)\r\n",
      "Requirement already satisfied: fastparquet in ./venv/lib64/python3.11/site-packages (from exllamav2) (2024.2.0)\r\n",
      "Requirement already satisfied: torch>=2.0.1 in ./venv/lib64/python3.11/site-packages (from exllamav2) (2.1.2)\r\n",
      "Requirement already satisfied: safetensors>=0.3.2 in ./venv/lib64/python3.11/site-packages (from exllamav2) (0.4.2)\r\n",
      "Requirement already satisfied: sentencepiece>=0.1.97 in ./venv/lib64/python3.11/site-packages (from exllamav2) (0.1.99)\r\n",
      "Requirement already satisfied: pygments in ./venv/lib/python3.11/site-packages (from exllamav2) (2.17.2)\r\n",
      "Requirement already satisfied: websockets in ./venv/lib64/python3.11/site-packages (from exllamav2) (12.0)\r\n",
      "Requirement already satisfied: regex in ./venv/lib64/python3.11/site-packages (from exllamav2) (2023.12.25)\r\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from torch>=2.0.1->exllamav2) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.11/site-packages (from torch>=2.0.1->exllamav2) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.11/site-packages (from torch>=2.0.1->exllamav2) (1.12)\r\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.11/site-packages (from torch>=2.0.1->exllamav2) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch>=2.0.1->exllamav2) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.11/site-packages (from torch>=2.0.1->exllamav2) (2023.10.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch>=2.0.1->exllamav2) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch>=2.0.1->exllamav2) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch>=2.0.1->exllamav2) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./venv/lib/python3.11/site-packages (from torch>=2.0.1->exllamav2) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.11/site-packages (from torch>=2.0.1->exllamav2) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.11/site-packages (from torch>=2.0.1->exllamav2) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.11/site-packages (from torch>=2.0.1->exllamav2) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.11/site-packages (from torch>=2.0.1->exllamav2) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.11/site-packages (from torch>=2.0.1->exllamav2) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./venv/lib/python3.11/site-packages (from torch>=2.0.1->exllamav2) (2.18.1)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch>=2.0.1->exllamav2) (12.1.105)\r\n",
      "Requirement already satisfied: triton==2.1.0 in ./venv/lib64/python3.11/site-packages (from torch>=2.0.1->exllamav2) (2.1.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.1->exllamav2) (12.3.101)\r\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./venv/lib64/python3.11/site-packages (from fastparquet->exllamav2) (1.24.4)\r\n",
      "Requirement already satisfied: cramjam>=2.3 in ./venv/lib64/python3.11/site-packages (from fastparquet->exllamav2) (2.8.1)\r\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from fastparquet->exllamav2) (23.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas->exllamav2) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas->exllamav2) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.11/site-packages (from pandas->exllamav2) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->exllamav2) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib64/python3.11/site-packages (from jinja2->torch>=2.0.1->exllamav2) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.11/site-packages (from sympy->torch>=2.0.1->exllamav2) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install exllamav2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T14:12:08.496474587Z",
     "start_time": "2024-02-15T14:12:07.324382244Z"
    }
   },
   "id": "917e18a8d5b84248",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "wizard_math_path = snapshot_download(repo_id=\"LoneStriker/WizardMath-7B-V1.1-4.0bpw-h6-exl2\", revision=\"main\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed61285699894d2e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/jt/.cache/torch_extensions/py311_cu121/exllamav2_ext/exllamav2_ext.so: undefined symbol: _ZN3c107WarningC1ESt7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEERKNS_14SourceLocationESsb",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexllamav2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m(\n\u001B[1;32m      2\u001B[0m     ExLlamaV2,\n\u001B[1;32m      3\u001B[0m     ExLlamaV2Config,\n\u001B[1;32m      4\u001B[0m     ExLlamaV2Cache,\n\u001B[1;32m      5\u001B[0m     ExLlamaV2Tokenizer,\n\u001B[1;32m      6\u001B[0m )\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexllamav2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgenerator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      9\u001B[0m     ExLlamaV2Sampler, ExLlamaV2StreamingGenerator\n\u001B[1;32m     10\u001B[0m )\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Erstellung des Modells\u001B[39;00m\n",
      "File \u001B[0;32m~/work/gdsc/open_source_llms/venv/lib/python3.11/site-packages/exllamav2/__init__.py:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexllamav2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexllamav2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ExLlamaV2\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexllamav2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcache\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ExLlamaV2CacheBase\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexllamav2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcache\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ExLlamaV2Cache\n",
      "File \u001B[0;32m~/work/gdsc/open_source_llms/venv/lib/python3.11/site-packages/exllamav2/model.py:25\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmath\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexllamav2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ExLlamaV2Config\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexllamav2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcache\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ExLlamaV2CacheBase\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexllamav2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinear\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ExLlamaV2Linear\n",
      "File \u001B[0;32m~/work/gdsc/open_source_llms/venv/lib/python3.11/site-packages/exllamav2/config.py:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexllamav2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfasttensors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m STFile\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mglob\u001B[39;00m\u001B[38;5;241m,\u001B[39m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mExLlamaV2Config\u001B[39;00m:\n",
      "File \u001B[0;32m~/work/gdsc/open_source_llms/venv/lib/python3.11/site-packages/exllamav2/fasttensors.py:5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexllamav2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m exllamav2_ext \u001B[38;5;28;01mas\u001B[39;00m ext_c\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert_dtype\u001B[39m(dt: \u001B[38;5;28mstr\u001B[39m):\n",
      "File \u001B[0;32m~/work/gdsc/open_source_llms/venv/lib/python3.11/site-packages/exllamav2/ext.py:152\u001B[0m\n\u001B[1;32m    148\u001B[0m     sources \u001B[38;5;241m=\u001B[39m [os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(sources_dir, s) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m sources_]\n\u001B[1;32m    150\u001B[0m     \u001B[38;5;66;03m# Load extension\u001B[39;00m\n\u001B[0;32m--> 152\u001B[0m     exllamav2_ext \u001B[38;5;241m=\u001B[39m \u001B[43mload\u001B[49m\u001B[43m \u001B[49m\u001B[43m\\\u001B[49m\n\u001B[1;32m    153\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mextension_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[43msources\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43msources\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_include_paths\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43msources_dir\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_ldflags\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mextra_ldflags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_cuda_cflags\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mextra_cuda_cflags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_cflags\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mextra_cflags\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m ext_c \u001B[38;5;241m=\u001B[39m exllamav2_ext\n\u001B[1;32m    166\u001B[0m \u001B[38;5;66;03m# Dummy tensor to pass to C++ extension in place of None/NULL\u001B[39;00m\n",
      "File \u001B[0;32m~/work/gdsc/open_source_llms/venv/lib64/python3.11/site-packages/torch/utils/cpp_extension.py:1308\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001B[0m\n\u001B[1;32m   1216\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(name,\n\u001B[1;32m   1217\u001B[0m          sources: Union[\u001B[38;5;28mstr\u001B[39m, List[\u001B[38;5;28mstr\u001B[39m]],\n\u001B[1;32m   1218\u001B[0m          extra_cflags\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1226\u001B[0m          is_standalone\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1227\u001B[0m          keep_intermediates\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m   1228\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m   1229\u001B[0m \u001B[38;5;124;03m    Loads a PyTorch C++ extension just-in-time (JIT).\u001B[39;00m\n\u001B[1;32m   1230\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1306\u001B[0m \u001B[38;5;124;03m        ...     verbose=True)\u001B[39;00m\n\u001B[1;32m   1307\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n\u001B[0;32m-> 1308\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_jit_compile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1309\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1310\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\u001B[43msources\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msources\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msources\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1311\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_cflags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1312\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_cuda_cflags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1313\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_ldflags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1314\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_include_paths\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1315\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbuild_directory\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_get_build_directory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1316\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1317\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwith_cuda\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1318\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_python_module\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1319\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_standalone\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1320\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_intermediates\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_intermediates\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/work/gdsc/open_source_llms/venv/lib64/python3.11/site-packages/torch/utils/cpp_extension.py:1736\u001B[0m, in \u001B[0;36m_jit_compile\u001B[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001B[0m\n\u001B[1;32m   1733\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_standalone:\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _get_exec_path(name, build_directory)\n\u001B[0;32m-> 1736\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_import_module_from_library\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuild_directory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_python_module\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/work/gdsc/open_source_llms/venv/lib64/python3.11/site-packages/torch/utils/cpp_extension.py:2136\u001B[0m, in \u001B[0;36m_import_module_from_library\u001B[0;34m(module_name, path, is_python_module)\u001B[0m\n\u001B[1;32m   2134\u001B[0m spec \u001B[38;5;241m=\u001B[39m importlib\u001B[38;5;241m.\u001B[39mutil\u001B[38;5;241m.\u001B[39mspec_from_file_location(module_name, filepath)\n\u001B[1;32m   2135\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m spec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2136\u001B[0m module \u001B[38;5;241m=\u001B[39m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodule_from_spec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspec\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2137\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(spec\u001B[38;5;241m.\u001B[39mloader, importlib\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mLoader)\n\u001B[1;32m   2138\u001B[0m spec\u001B[38;5;241m.\u001B[39mloader\u001B[38;5;241m.\u001B[39mexec_module(module)\n",
      "\u001B[0;31mImportError\u001B[0m: /home/jt/.cache/torch_extensions/py311_cu121/exllamav2_ext/exllamav2_ext.so: undefined symbol: _ZN3c107WarningC1ESt7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEERKNS_14SourceLocationESsb"
     ]
    }
   ],
   "source": [
    "from exllamav2 import(\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Config,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Tokenizer,\n",
    ")\n",
    "\n",
    "from exllamav2.generator import (\n",
    "    ExLlamaV2Sampler, ExLlamaV2StreamingGenerator\n",
    ")\n",
    "\n",
    "# Erstellung des Modells\n",
    "config = ExLlamaV2Config() # Einstellungen für das Modell zum Beispiel welches Modell verwendet werden soll oder die maximale Sequenz Länge des LLMs\n",
    "config.model_dir = wizard_math_path\n",
    "config.prepare()\n",
    "model = ExLlamaV2(config) \n",
    "\n",
    "cache = ExLlamaV2Cache(model, lazy=True)\n",
    "model.load_autosplit(cache)\n",
    "\n",
    "# Erstellung des Tokenizers\n",
    "tokenizer = ExLlamaV2Tokenizer(config)\n",
    "generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)\n",
    "generator.set_stop_conditions([tokenizer.eos_token_id])\n",
    "\n",
    "# Einstellungen für die Anfrage\n",
    "settings = ExLlamaV2Sampler.Settings() # Einstellungen für die Anfrage zum Beispiel die Temperatur oder die maximale Anzahl an Tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T14:18:10.631239493Z",
     "start_time": "2024-02-15T14:18:10.557711012Z"
    }
   },
   "id": "a627cf50f1c9d475",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "Meredith is a freelance blogger who writes about health topics and submits to clients each\n",
    "day as her permanent job. A blog article takes an average of 4 hours to research and write about. Last week,\n",
    "she wrote 5 articles on Monday and 2/5 times more articles on Tuesday than on Monday. On Wednesday,\n",
    "she wrote twice the number of articles she wrote on Tuesday. Calculate the total number of hours she spent\n",
    "writing articles in the three days.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Below is an instruction that describes a task. Write a\n",
    "response that appropriately completes the request.\\n\\n###\n",
    "Instruction:\\n{instruction}\\n\\n### Response:\n",
    "\"\"\"\n",
    "\n",
    "# Umwandlung der Anfrage in Tokens\n",
    "instruction_ids = tokenizer.encode(prompt, add_bos=True) # add bos token sorgt dafür, dass der start token hinzugefügt wird\n",
    "generator.begin_stream(instruction_ids, settings)\n",
    "\n",
    "while True:\n",
    "    chunk, eos, _ = generator.stream()\n",
    "    if eos:\n",
    "        break\n",
    "    print(chunk, end=\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2a4ddae5b5a8f4a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ff0c6bb969c96cc9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
