{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Downloaden der GGUF Modelle\n",
    "Um zum ersten Mal ein LLM bei uns auszuführen, wollen wir zunächst ein gguf Modelle verwenden. Für den Anfang verwenden wir das Llama-2-7B-GGUF und das Mistral-7B-Instruct-v0.1-GGUF Modell. Diese Modelle sind relativ klein und können direkt auf einer CPU ausgeführt werden. Um diser Modelle zu verwenden, müssen wir diese zunächst herunterladen. Dafür verwenden wir die Funktion hf_hub_download aus dem huggingface_hub Modul um die Modelle von huggingface herunterzuladen. Weitere gguf Modelle können zum Beispiel  [hier](https://huggingface.co/TheBloke) gefunden werden."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3369a0a1ee208f8a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "llama_path = hf_hub_download(repo_id=\"TheBloke/Llama-2-7B-GGUF\", filename=\"llama-2-7b.Q4_0.gguf\")\n",
    "mistral_path = hf_hub_download(repo_id=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "                               filename=\"mistral-7b-instruct-v0.1.Q4_0.gguf\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f883f132125d01d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ausführen der gguf Modelle\n",
    "Nach dem herunterladen der Modelle, können wir diese nun in unserem Python Code verwenden. Dafür verwenden wir die llama-cpp-python Bibliothek, die uns eine einfache Schnittstelle zu den Modellen bietet. Um ein Modell zu verwenden, müssen wir zunächst eine Instanz der Llama Klasse erstellen und den Pfad zum Modell übergeben. Anschließend können wir die Instanz wie eine Funktion aufrufen und die Eingabe (Prompt) übergeben. Die Ausgabe ist ein Python Dictionary, das die Antwort des Modells enthält."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20dc170c4f08a75d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llama = Llama(llama_path)\n",
    "response = llama(\"What is the capital of France?\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e36ffe6542e1dae",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die Ausgabe ist eine Python Map. Wenn wir diese etwas schöner formattieren könnte die Ausgabe wie folgt aussehen.\n",
    "\n",
    "```\n",
    "{\n",
    "  \"id\": \"cmpl-97de95d4-101f-4193-a544-65d26211ddba\",\n",
    "  \"object\": \"text_completion\",\n",
    "  \"created\": 1707901428,\n",
    "  \"model\": \"/home/jt/.cache/huggingface/hub/models--TheBloke--Llama-2-7B-GGUF/snapshots/b4e04e128f421c93a5f1e34ac4d7ca9b0af47b80/llama-2-7b.Q4_0.gguf\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"text\": \"1. hopefully you got a good mark in your Geography lesson today!\\n\",\n",
    "      \"index\": 0,\n",
    "      \"logprobs\": \"None\",\n",
    "      \"finish_reason\": \"length\"\n",
    "    }\n",
    "  ],\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 13,\n",
    "    \"completion_tokens\": 16,\n",
    "    \"total_tokens\": 29\n",
    "  }\n",
    "}\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bedf39529f332c92"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zugriff auf die Antwort"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12706d37d00185ab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(response[\"choices\"][0][\"text\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2098d4af27e8451",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model mit GPU Unterstützung\n",
    "Aktuell ist unsere Modell noch etwas langsam um dies nun zu beschleunigen wollen wir eine GPU nutzen. Um unser Model auf der GPU auszuführen, müssen wir zunächst den Laufzeit-Typ auf GPU umstellen. Dafür klicken wir in der Toolbar unter Laufzeit auf Laufzeittyp ändern und wählen nun die T4 GPU aus. Anschließend installieren wir llama-cpp-python erneut, diesmal mit der Option -DLLAMA_CUBLAS=on um CUDA zu aktivieren."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d1d73ac514f9e16"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46dd86fda36fa972",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llama = Llama(\n",
    "    llama_path,\n",
    "    n_gpu_layers=-1,  # Verschiebt die Berechnung auf die GPU\n",
    "    verbose=False,  # Entfernt die Logausgaben\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84d814bf34d0bf1b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = llama(\"What is the capital of France?\")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e21cc17d5b75dbec",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bessere Antworten bekommen\n",
    "Das Llama Model was wir verwenden ist ein sogenanntes Foundation Model. Diese Modelle sind meißt nicht direkt in der Anwendung nutzbar, sondern müssen noch finegetuned werden. Ein soein gefinetuntes Modell ist das Mistral-7B-Instruct-v0.1-GGUF Modell. Dieses Modell ist speziell darauf trainiert worden, Anweisungen zu befolgen. Dieses Modell haben wir bereits heruntergeladen und können es nun verwenden. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "677ff76b567bbfe0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mistral = Llama(\n",
    "    mistral_path,\n",
    "    n_gpu_layers=-1,\n",
    "    verbose=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef5715cb6adc58b3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = mistral(\"What is the capital of France?\")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9dc443f3557e7e9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = mistral(\n",
    "    \"Write me a poem about the technology of the future.\",\n",
    "    max_tokens=512,\n",
    ")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3129491fd3dc1030",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chatbot erstellung\n",
    "Wir wollen nun einen Chatbot erstellen welche die bishere Konversation speichert und auf diese aufbaut. Dafür nutzen wir die create_chat_completion Funktion von llama-cpp-python. Diese Funktion nimmt die bisherige Konversation und die Anfrage entgegen und gibt die Antwort zurück."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f09497ddac5d0a98"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = mistral.create_chat_completion([\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the capital of France?\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Paris\",\n",
    "    }\n",
    "    ,\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Germany?\",\n",
    "    }\n",
    "], max_tokens=512)\n",
    "\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9add8d2ea646a79f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "current_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"answer in the style of a pirate\",\n",
    "    }\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    current_messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_input,\n",
    "    })\n",
    "\n",
    "    response = mistral.create_chat_completion(current_messages, max_tokens=512)\n",
    "    current_messages.append(response[\"choices\"][0][\"message\"])\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1be6db17042728f7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Streaming der Antworten\n",
    "Um die Antworten des Modells in Echtzeit zu erhalten, können wir die Streaming Funktion von llama-cpp-python verwenden. Dafür müssen wir lediglich den stream Parameter auf True setzen. Anschließend können wir die Antworten des Modells in Echtzeit erhalten."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93f668bec992446f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = mistral(\n",
    "    \"Write me a poem about the technology of the future.\",\n",
    "    max_tokens=512,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for message in response:\n",
    "    print(message[\"choices\"][0][\"text\"], end=\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae44560be9333686",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aufgabe\n",
    "baut das Streaming in unseren Chatbot ein"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fa8ccae2f93dd5b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "current_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"answer in the style of a pirate\",\n",
    "    }\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    current_messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_input,\n",
    "    })\n",
    "\n",
    "    response = mistral.create_chat_completion(\n",
    "        current_messages,\n",
    "        max_tokens=512,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    message = \"\"\n",
    "    for chunk in response:\n",
    "        delta = chunk[\"choices\"][0][\"delta\"]\n",
    "        if 'content' in delta:\n",
    "          part = delta[\"content\"]\n",
    "          message += part\n",
    "          print(part, end=\"\")\n",
    "    print()\n",
    "    current_messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": message,\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ac816855b548835"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "del mistral\n",
    "del llama"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "749bc104f7356b01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exllamav2\n",
    "Wir wollen uns nun eine weiter Quantisierung von LLM Modellen anschauen. Dafür verwenden wir das Exllamav2, welches exl2 Modelle verwendet.\n",
    "\n",
    "Das besondere an Exllamav2 ist, ist das ist sehr schnell ist, weshalb wir es uns nun im folgenden anschauen wollen. Zunächst müssten wir hierfür exllamav2 installieren und ein Modell herunterladen. Viele Modelle können zum Beispiel [hier](https://huggingface.co/LoneStriker) gefunden werden."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3970fbc44634d7d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install exllamav2 --upgrade --force-reinstall --no-cache-dir"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "917e18a8d5b84248",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "wizard_math_path = snapshot_download(repo_id=\"LoneStriker/WizardMath-7B-V1.1-4.0bpw-h6-exl2\", revision=\"main\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed61285699894d2e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from exllamav2 import (\n",
    "    ExLlamaV2,\n",
    "    ExLlamaV2Config,\n",
    "    ExLlamaV2Cache,\n",
    "    ExLlamaV2Tokenizer,\n",
    ")\n",
    "\n",
    "from exllamav2.generator import (\n",
    "    ExLlamaV2Sampler, ExLlamaV2StreamingGenerator\n",
    ")\n",
    "\n",
    "# Erstellung des Modells\n",
    "config = ExLlamaV2Config()  # Einstellungen für das Modell zum Beispiel welches Modell verwendet werden soll oder die maximale Sequenz Länge des LLMs\n",
    "config.model_dir = wizard_math_path\n",
    "config.prepare()\n",
    "model = ExLlamaV2(config)\n",
    "\n",
    "cache = ExLlamaV2Cache(model, lazy=True)\n",
    "model.load_autosplit(cache)\n",
    "\n",
    "# Erstellung des Tokenizers\n",
    "tokenizer = ExLlamaV2Tokenizer(config)\n",
    "generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)\n",
    "generator.set_stop_conditions([tokenizer.eos_token_id])\n",
    "\n",
    "# Einstellungen für die Anfrage\n",
    "settings = ExLlamaV2Sampler.Settings()  # Einstellungen für die Anfrage zum Beispiel die Temperatur oder die maximale Anzahl an Tokens"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a627cf50f1c9d475",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "instruction = \"\"\"\n",
    "Meredith is a freelance blogger who writes about health topics and submits to clients each\n",
    "day as her permanent job. A blog article takes an average of 4 hours to research and write about. Last week,\n",
    "she wrote 5 articles on Monday and 2/5 times more articles on Tuesday than on Monday. On Wednesday,\n",
    "she wrote twice the number of articles she wrote on Tuesday. Calculate the total number of hours she spent\n",
    "writing articles in the three days.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Below is an instruction that describes a task. Write a\n",
    "response that appropriately completes the request.\\n\\n###\n",
    "Instruction:\\n{instruction}\\n\\n### Response:\n",
    "\"\"\"\n",
    "\n",
    "# Umwandlung der Anfrage in Tokens\n",
    "instruction_ids = tokenizer.encode(prompt,\n",
    "                                   add_bos=True)  # add bos token sorgt dafür, dass der start token hinzugefügt wird\n",
    "generator.begin_stream(instruction_ids, settings)\n",
    "\n",
    "while True:\n",
    "    chunk, eos, _ = generator.stream()\n",
    "    if eos:\n",
    "        break\n",
    "    print(chunk, end=\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2a4ddae5b5a8f4a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ff0c6bb969c96cc9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
